{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orion.data import load_signal, load_anomalies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_name = 'multivariate/S-1'\n",
    "\n",
    "data = load_signal(signal_name)\n",
    "data = data[8000:10000]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlblocks import MLPipeline\n",
    "\n",
    "pipeline_name = 'chronos2'\n",
    "\n",
    "pipeline = MLPipeline(pipeline_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "The Chronos2 pipeline can be customized with the following hyperparameters:\n",
    "\n",
    "| Primitive | Parameter | Default | Description |\n",
    "|-----------|-----------|---------|-------------|\n",
    "| time_segments_aggregate | `interval` | 21600 | Aggregation interval in seconds |\n",
    "| time_segments_aggregate | `method` | \"mean\" | Aggregation method (mean, median, sum) |\n",
    "| rolling_window_sequences | `window_size` | 250 | Context window size |\n",
    "| **Chronos2** | `pred_len` | 1 | Prediction horizon length |\n",
    "| **Chronos2** | `batch_size` | 32 | Batch size for inference |\n",
    "| **Chronos2** | `target` | 0 | Target column index (multivariate) |\n",
    "| find_anomalies | `window_size_portion` | 0.33 | Portion of data for window |\n",
    "| find_anomalies | `fixed_threshold` | True | Use fixed vs dynamic threshold |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"mlstars.custom.timeseries_preprocessing.time_segments_aggregate#1\": {\n",
    "        \"time_column\": \"timestamp\",\n",
    "        \"interval\": 21600,        \n",
    "        \"method\": \"mean\"        \n",
    "    },\n",
    "    \n",
    "    \"mlstars.custom.timeseries_preprocessing.rolling_window_sequences#1\": {\n",
    "        \"target_column\": 0,\n",
    "        \"window_size\": 250\n",
    "    },\n",
    "    \n",
    "    \"orion.primitives.chronos2.Chronos2#1\": {\n",
    "        \"pred_len\": 1,          \n",
    "        \"batch_size\": 32,       \n",
    "        \"target\": 0,            \n",
    "    },\n",
    "    \n",
    "    \"orion.primitives.timeseries_anomalies.find_anomalies#1\": {\n",
    "        \"window_size_portion\": 0.33,\n",
    "        \"window_step_size_portion\": 0.1,\n",
    "        \"fixed_threshold\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "pipeline.set_hyperparameters(hyperparameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step by step execution\n",
    "\n",
    "MLPipelines are compose of a squence of primitives, these primitives apply tranformation and calculation operations to the data and updates the variables within the pipeline. To view the primitives used by the pipeline, we access its `primtivies` attribute. \n",
    "\n",
    "The `UniTS` contains 6 primitives. we will observe how the `context` (which are the variables held within the pipeline) are updated after the execution of each primitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.primitives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### time segments aggregate\n",
    "this primitive creates an equi-spaced time series by aggregating values over fixed specified interval.\n",
    "\n",
    "* **input**: `X` which is an n-dimensional sequence of values.\n",
    "* **output**:\n",
    "    - `X` sequence of aggregated values, one column for each aggregation method.\n",
    "    - `index` sequence of index values (first index of each aggregated segment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = pipeline.fit(data, output_=0)\n",
    "context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in list(zip(context['index'], context['X']))[:5]:\n",
    "    print(\"entry at {} has value {}\".format(i, x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleImputer\n",
    "this primitive is an imputation transformer for filling missing values.\n",
    "* **input**: `X` which is an n-dimensional sequence of values.\n",
    "* **output**: `X` which is a transformed version of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "\n",
    "context = pipeline.fit(**context, output_=step, start_=step)\n",
    "context.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rolling window sequence\n",
    "this primitive generates many sub-sequences of the original sequence. it uses a rolling window approach to create the sub-sequences out of time series data.\n",
    "\n",
    "* **input**: \n",
    "    - `X` n-dimensional sequence to iterate over.\n",
    "    - `index` array containing the index values of X.\n",
    "* **output**:\n",
    "    - `X` input sequences.\n",
    "    - `y` target sequences.\n",
    "    - `index` first index value of each input sequence.\n",
    "    - `target_index` first index value of each target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 2\n",
    "\n",
    "context = pipeline.fit(**context, output_=step, start_=step)\n",
    "context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after slicing X into multiple sub-sequences\n",
    "# we obtain a 3 dimensional matrix X where\n",
    "# the shape indicates (# slices, window size, 1)\n",
    "# and similarly y is (# slices, target size)\n",
    "\n",
    "print(\"X shape = {}\\ny shape = {}\\nindex shape = {}\\ntarget index shape = {}\".format(\n",
    "    context['X'].shape, context['y'].shape, context['index'].shape, context['target_index'].shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chronos-2\n",
    "This is the forecasting step using Amazon Chronos-2 Time Series Foundation Model. You can read more about it in the [related paper](https://arxiv.org/abs/2510.15821). The [Huggingface Repo](https://huggingface.co/amazon/chronos-2) has additional helpful information about the use of the model. This is a multivariate model that does single target predictions.\n",
    "\n",
    "* **input**: \n",
    "    - `X` n-dimensional array containing the input sequences for the model.\n",
    "* **output**: \n",
    "    - `y_hat` predicted values for target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 3\n",
    "\n",
    "context = pipeline.fit(**context, output_=step, start_=step)\n",
    "context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context['y_hat'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression errors\n",
    "\n",
    "this primitive computes an array of errors comparing predictions and expected output.\n",
    "\n",
    "* **input**: \n",
    "    - `y` ground truth.\n",
    "    - `y_hat` forecasted values.\n",
    "* **output**: `errors` array of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 4\n",
    "\n",
    "context = pipeline.fit(**context, output_=step, start_=step)\n",
    "context.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find anomalies\n",
    "\n",
    "this primitive finds anomalies from sequence of errors\n",
    "\n",
    "* **input**: \n",
    "    - `errors` array of errors\n",
    "    - `target_index` indices\n",
    "* **output**: `anomalies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 5\n",
    "\n",
    "context = pipeline.fit(**context, output_=step, start_=step)\n",
    "context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context['anomalies']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate performance\n",
    "\n",
    "In this next step we will load some already known anomalous intervals and evaluate how\n",
    "good our anomaly detection was by comparing those with our detected intervals.\n",
    "\n",
    "For this, we will first load the known anomalies for the signal that we are using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orion.data import load_anomalies\n",
    "\n",
    "ground_truth = load_anomalies('S-1')\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = []\n",
    "for ano in context['anomalies']:\n",
    "    anomalies.append((ano[0], ano[1]))\n",
    "anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orion.evaluation import contextual_confusion_matrix, contextual_f1_score\n",
    "\n",
    "start, end = context['index'][0], context['index'][-1]\n",
    "\n",
    "contextual_confusion_matrix(ground_truth, anomalies, start = start, end = end, weighted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_f1_score(ground_truth, anomalies, start = start, end = end, weighted=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_orion_dependencies",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
