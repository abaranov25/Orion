{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orion.data import load_signal, load_anomalies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1222819200</td>\n",
       "      <td>-0.366359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1222840800</td>\n",
       "      <td>-0.394108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1222862400</td>\n",
       "      <td>0.403625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1222884000</td>\n",
       "      <td>-0.362759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1222905600</td>\n",
       "      <td>-0.370746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp     value\n",
       "0  1222819200 -0.366359\n",
       "1  1222840800 -0.394108\n",
       "2  1222862400  0.403625\n",
       "3  1222884000 -0.362759\n",
       "4  1222905600 -0.370746"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_name = 'S-1'\n",
    "\n",
    "data = load_signal(signal_name)\n",
    "\n",
    "anomalies = load_anomalies(signal_name)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baranov/miniconda/envs/orion310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-02-10 01:37:01.358594: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-02-10 01:37:01.392927: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-02-10 01:37:01.392954: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-02-10 01:37:01.392990: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-02-10 01:37:01.400419: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-10 01:37:02.090769: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from mlblocks import MLPipeline\n",
    "\n",
    "pipeline_name = 'chronos2'\n",
    "\n",
    "pipeline = MLPipeline(pipeline_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "The Chronos2 pipeline can be customized with the following hyperparameters:\n",
    "\n",
    "| Primitive | Parameter | Default | Description |\n",
    "|-----------|-----------|---------|-------------|\n",
    "| time_segments_aggregate | `interval` | 600 | Aggregation interval in seconds |\n",
    "| time_segments_aggregate | `method` | \"mean\" | Aggregation method (mean, median, sum) |\n",
    "| rolling_window_sequences | `window_size` | 256 | Context window size |\n",
    "| **Chronos2** | `window_size` | 256 | Must match rolling_window_sequences |\n",
    "| **Chronos2** | `pred_len` | 1 | Prediction horizon length |\n",
    "| **Chronos2** | `repo_id` | \"amazon/chronos-2\" | HuggingFace model repository |\n",
    "| **Chronos2** | `batch_size` | 32 | Batch size for inference |\n",
    "| **Chronos2** | `target` | 0 | Target column index (multivariate) |\n",
    "| **Chronos2** | `time_interval` | 600 | Time interval between samples (seconds) |\n",
    "| find_anomalies | `window_size_portion` | 0.33 | Portion of data for window |\n",
    "| find_anomalies | `fixed_threshold` | True | Use fixed vs dynamic threshold |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"mlstars.custom.timeseries_preprocessing.time_segments_aggregate#1\": {\n",
    "        \"time_column\": \"timestamp\",\n",
    "        \"interval\": 600,        \n",
    "        \"method\": \"mean\"        \n",
    "    },\n",
    "    \n",
    "    \"mlstars.custom.timeseries_preprocessing.rolling_window_sequences#1\": {\n",
    "        \"target_column\": 0,\n",
    "        \"window_size\": 256      \n",
    "    },\n",
    "    \n",
    "    \"orion.primitives.chronos2.Chronos2#1\": {\n",
    "        \"window_size\": 256,     \n",
    "        \"pred_len\": 1,          \n",
    "        \"repo_id\": \"amazon/chronos-2\",  \n",
    "        \"batch_size\": 32,       \n",
    "        \"target\": 0,            \n",
    "        \"time_interval\": 600    \n",
    "    },\n",
    "    \n",
    "    \"orion.primitives.timeseries_anomalies.find_anomalies#1\": {\n",
    "        \"window_size_portion\": 0.33,\n",
    "        \"window_step_size_portion\": 0.1,\n",
    "        \"fixed_threshold\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "pipeline.set_hyperparameters(hyperparameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step by step execution\n",
    "\n",
    "MLPipelines are compose of a squence of primitives, these primitives apply tranformation and calculation operations to the data and updates the variables within the pipeline. To view the primitives used by the pipeline, we access its `primtivies` attribute. \n",
    "\n",
    "The `UniTS` contains 6 primitives. we will observe how the `context` (which are the variables held within the pipeline) are updated after the execution of each primitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mlstars.custom.timeseries_preprocessing.time_segments_aggregate',\n",
       " 'sklearn.impute.SimpleImputer',\n",
       " 'mlstars.custom.timeseries_preprocessing.rolling_window_sequences',\n",
       " 'orion.primitives.chronos2.Chronos2',\n",
       " 'orion.primitives.timeseries_errors.regression_errors',\n",
       " 'orion.primitives.timeseries_anomalies.find_anomalies']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.primitives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### time segments aggregate\n",
    "this primitive creates an equi-spaced time series by aggregating values over fixed specified interval.\n",
    "\n",
    "* **input**: `X` which is an n-dimensional sequence of values.\n",
    "* **output**:\n",
    "    - `X` sequence of aggregated values, one column for each aggregation method.\n",
    "    - `index` sequence of index values (first index of each aggregated segment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X', 'index'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = pipeline.fit(data, output_=0)\n",
    "context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry at 1222819200 has value [-0.36635895]\n",
      "entry at 1222819800 has value [nan]\n",
      "entry at 1222820400 has value [nan]\n",
      "entry at 1222821000 has value [nan]\n",
      "entry at 1222821600 has value [nan]\n"
     ]
    }
   ],
   "source": [
    "for i, x in list(zip(context['index'], context['X']))[:5]:\n",
    "    print(\"entry at {} has value {}\".format(i, x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleImputer\n",
    "this primitive is an imputation transformer for filling missing values.\n",
    "* **input**: `X` which is an n-dimensional sequence of values.\n",
    "* **output**: `X` which is a transformed version of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['index', 'X'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 1\n",
    "\n",
    "context = pipeline.fit(**context, output_=step, start_=step)\n",
    "context.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rolling window sequence\n",
    "this primitive generates many sub-sequences of the original sequence. it uses a rolling window approach to create the sub-sequences out of time series data.\n",
    "\n",
    "* **input**: \n",
    "    - `X` n-dimensional sequence to iterate over.\n",
    "    - `index` array containing the index values of X.\n",
    "* **output**:\n",
    "    - `X` input sequences.\n",
    "    - `y` target sequences.\n",
    "    - `index` first index value of each input sequence.\n",
    "    - `target_index` first index value of each target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['index', 'X', 'y', 'target_index'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 2\n",
    "\n",
    "context = pipeline.fit(**context, output_=step, start_=step)\n",
    "context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape = (365073, 256, 1)\n",
      "y shape = (365073, 1)\n",
      "index shape = (365073,)\n",
      "target index shape = (365073,)\n"
     ]
    }
   ],
   "source": [
    "# after slicing X into multiple sub-sequences\n",
    "# we obtain a 3 dimensional matrix X where\n",
    "# the shape indicates (# slices, window size, 1)\n",
    "# and similarly y is (# slices, target size)\n",
    "\n",
    "print(\"X shape = {}\\ny shape = {}\\nindex shape = {}\\ntarget index shape = {}\".format(\n",
    "    context['X'].shape, context['y'].shape, context['index'].shape, context['target_index'].shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chronos-2\n",
    "This is the forecasting step using Amazon Chronos-2 Time Series Foundation Model. You can read more about it in the [related paper](https://arxiv.org/abs/2510.15821). The [Huggingface Repo](https://huggingface.co/amazon/chronos-2) has additional helpful information about the use of the model. This is a multivariate model that does single target predictions.\n",
    "\n",
    "* **input**: \n",
    "    - `X` n-dimensional array containing the input sequences for the model.\n",
    "* **output**: \n",
    "    - `y_hat` predicted values for target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['index', 'target_index', 'X', 'y', 'y_hat'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 3\n",
    "\n",
    "context = pipeline.fit(**context, output_=step, start_=step)\n",
    "context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365073, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context['y_hat'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression errors\n",
    "\n",
    "this primitive computes an array of errors comparing predictions and expected output.\n",
    "\n",
    "* **input**: \n",
    "    - `y` ground truth.\n",
    "    - `y_hat` forecasted values.\n",
    "* **output**: `errors` array of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['index', 'target_index', 'y_hat', 'X', 'y', 'errors'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 4\n",
    "\n",
    "context = pipeline.fit(**context, output_=step, start_=step)\n",
    "context.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find anomalies\n",
    "\n",
    "this primitive finds anomalies from sequence of errors\n",
    "\n",
    "* **input**: \n",
    "    - `errors` array of errors\n",
    "    - `target_index` indices\n",
    "* **output**: `anomalies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['index', 'target_index', 'y_hat', 'errors', 'X', 'y', 'anomalies'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 5\n",
    "\n",
    "context = pipeline.fit(**context, output_=step, start_=step)\n",
    "context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2229836e+09, 1.2231516e+09, 5.3378149e-01]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context['anomalies']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate performance\n",
    "\n",
    "In this next step we will load some already known anomalous intervals and evaluate how\n",
    "good our anomaly detection was by comparing those with our detected intervals.\n",
    "\n",
    "For this, we will first load the known anomalies for the signal that we are using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1398168000</td>\n",
       "      <td>1407823200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        start         end\n",
       "0  1398168000  1407823200"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from orion.data import load_anomalies\n",
    "\n",
    "ground_truth = load_anomalies('S-1')\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1222983600.0, 1223151600.0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomalies = []\n",
    "for ano in context['anomalies']:\n",
    "    anomalies.append((ano[0], ano[1]))\n",
    "anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 1, 1, 0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from orion.evaluation import contextual_confusion_matrix, contextual_f1_score\n",
    "\n",
    "start, end = context['index'][0], context['index'][-1]\n",
    "\n",
    "contextual_confusion_matrix(ground_truth, anomalies, start = start, end = end, weighted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid value encountered for precision 0.0/ recall 0.0.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/baranov/projects/Orion/orion/evaluation/common.py\", line 70, in _f1_score\n",
      "    return 2 * (precision * recall) / (precision + recall)\n",
      "ZeroDivisionError: float division by zero\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_f1_score(ground_truth, anomalies, start = start, end = end, weighted=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orion310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
